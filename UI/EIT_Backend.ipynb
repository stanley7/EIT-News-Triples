{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOokEGLwAY59"
      },
      "outputs": [],
      "source": [
        "#!kill -9 $(lsof -t -i:5000) 2>/dev/null\n",
        "#!kill -9 $(lsof -t -i:5050) 2>/dev/null\n",
        "#!pkill -f flask 2>/dev/null || true\n",
        "#!pkill -f ngrok 2>/dev/null || true\n",
        "\n",
        "!pip install -q flask flask-cors pyngrok transformers accelerate torch rapidfuzz spacy spacy-llm beautifulsoup4 requests networkx ipysigma python-louvain pandas scikit-learn\n",
        "\n",
        "import torch, json, re, gc, time\n",
        "import os, tempfile\n",
        "import pathlib\n",
        "import networkx as nx\n",
        "from ipysigma import Sigma\n",
        "import community.community_louvain as community_louvain\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata\n",
        "from rapidfuzz import fuzz, process\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "actor_lists = {\n",
        "    \"EIT Organizations\": [\n",
        "        \"European Institute of Innovation and Technology (EIT)\",\n",
        "        \"EIT Food\", \"EIT Health\", \"EIT InnoEnergy\", \"EIT Digital\",\n",
        "        \"EIT RawMaterials\", \"EIT Manufacturing\", \"EIT Urban Mobility\",\n",
        "        \"EIT Climate-KIC\", \"EIT Culture & Creativity\"\n",
        "    ],\n",
        "    \"Universities / Research Institutes\": [\n",
        "        \"KU Leuven\", \"KTH Royal Institute of Technology\", \"Universitat Politècnica de Catalunya\",\n",
        "        \"Eindhoven University of Technology (TU/e)\", \"Instituto Superior Técnico\",\n",
        "        \"ESADE Business School\", \"Politecnico di Torino\", \"École Polytechnique\",\n",
        "        \"Aalto University\", \"Université Paris Sciences et Lettres (PSL)\", \"Université Paris-Saclay\",\n",
        "        \"Grenoble INP Institute of Technology\", \"École des Ponts ParisTech (ENPC)\",\n",
        "        \"AGH University of Science and Technology\", \"Imperial College London\",\n",
        "        \"University of Oxford\", \"University of Cambridge\", \"IESE Business School\",\n",
        "        \"IMIM – Hospital del Mar Medical Research Institute\",\n",
        "        \"Delft University of Technology\", \"University of Porto\",\n",
        "        \"University of Debrecen\", \"University of Luxembourg\",\n",
        "        \"Leitat Technology Center\", \"Fraunhofer Institute\",\n",
        "        \"Technical University of Munich\", \"RWTH Aachen University\",\n",
        "        \"Czech Technical University in Prague\",\n",
        "        \"Uppsala University\", \"Tartu University Hospital\",\n",
        "        \"North Lisbon University Hospital Centre\", \"Hospital Clínic\",\n",
        "        \"University of Maastricht\", \"Eötvös Lorand University\", \"KTH Institute\",\n",
        "        \"University of Pécs\"\n",
        "    ],\n",
        "    \"Companies / Corporates\": [\n",
        "        \"Schneider Electric\", \"Siemens Healthineers\", \"Roche\", \"Bayer\", \"Sanofi\",\n",
        "        \"Johnson & Johnson\", \"IBM\", \"Philips\", \"Bosch\", \"Microsoft\", \"GE Healthcare\",\n",
        "        \"URGO Group\", \"Ferrer\", \"Matmut\", \"ABB\", \"ENGIE\", \"Airbus\", \"ArcelorMittal\",\n",
        "        \"Veolia\", \"TotalEnergies\", \"Nestlé\", \"Danone\", \"PepsiCo\", \"Unilever\",\n",
        "        \"Shell\", \"Vattenfall\", \"Iberdrola\", \"Enel\", \"Equinor\", \"Nokia\", \"Ericsson\"\n",
        "    ],\n",
        "    \"Startups / SMEs\": [\n",
        "        \"iLoF\", \"Sleepiz\", \"Optellum\", \"Idoven\", \"PIPRA\", \"Antegenes\",\n",
        "        \"Clinomic\", \"Unhindr\", \"Leuko\", \"Ochre Bio\",\n",
        "        \"Hearts Radiant\", \"Allelica\", \"SolasCure\", \"Peptomyc\",\n",
        "        \"Oxford Endovascular\", \"Tubulis\", \"SideROS\", \"Emperra\",\n",
        "        \"FasTeesH\", \"MEDIKURA\", \"SpinDiag\", \"Selio Medical\",\n",
        "        \"Damibu\", \"Telomium\", \"Tracegrow\", \"Entremo\",\n",
        "        \"Recycleye\", \"InnoTractor\", \"LMAD\", \"OvaExpert\",\n",
        "        \"Feno\", \"Ganymed Robotics\",\n",
        "        \"ABLE Human Motion\", \"FLOWTION\", \"SeizeIT\", \"NanoRacks\",\n",
        "        \"AMEN New Technologies\", \"CroíValve\"\n",
        "    ],\n",
        "    \"Government / Public Sector\": [\n",
        "        \"European Commission\", \"European Union\", \"European Parliament\",\n",
        "        \"European Investment Fund (EIF)\", \"European Investment Bank (EIB)\",\n",
        "        \"National Health Service (UK)\", \"Ministry of Human Capacities (Hungary)\",\n",
        "        \"German Federal Ministry of Education and Research\", \"City of Debrecen\",\n",
        "        \"Spanish Ministry of Science and Innovation\",\n",
        "        \"French Ministry of Higher Education and Research\",\n",
        "        \"Italian Ministry for Economic Development\",\n",
        "        \"Polish Ministry of Climate and Environment\",\n",
        "        \"European Council\", \"European Court of Auditors\"\n",
        "    ],\n",
        "    \"Networks / Consortia / Foundations\": [\n",
        "        \"European Battery Alliance\", \"European Youth Energy Network\",\n",
        "        \"CommUnity+\", \"Foundation for Management and Industrial Research (MIR)\",\n",
        "        \"Enterprise Europe Network\", \"HealthTech For Care\",\n",
        "        \"Venture Centre of Excellence (VCoE)\",\n",
        "        \"BioMed Alliance\", \"WE Health Consortium\", \"CLOSE Consortium\",\n",
        "        \"InnoEnergy Alliance\", \"RawMaterials Academy\", \"Urban Mobility Academy\",\n",
        "        \"Climate-KIC Alumni Association\", \"EIT Digital Alumni\", \"RIS Hub Network\",\n",
        "        \"Regional Innovation Scheme (RIS)\", \"EIT Community Booster\",\n",
        "        \"Supernovas Programme\", \"Fondation de l'Avenir\",\n",
        "        \"Polish Medical Mission\", \"BRIGHT Project Innovation Team\"\n",
        "    ],\n",
        "    \"Investors / Funding Bodies\": [\n",
        "        \"Santander InnoEnergy Climate Fund\", \"EBA Strategic Battery Materials Fund\",\n",
        "        \"Aescuvest Crowdfunding Platform\", \"Zafir Capital\", \"Alta Life Sciences\",\n",
        "        \"YES!Delft\", \"Startup Wise Guys\", \"Speedinvest\", \"Seedcamp\",\n",
        "        \"EASME – Executive Agency for SMEs\", \"Horizon Europe\",\n",
        "        \"European Innovation Council (EIC)\", \"Business Finland\",\n",
        "        \"Vinnova\", \"CDTI Spain\", \"Bpifrance\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "ALL_ACTORS = []\n",
        "for category_actors in actor_lists.values():\n",
        "    ALL_ACTORS.extend(category_actors)\n",
        "\n",
        "ACTOR_LOWER_MAP = {actor.lower(): actor for actor in ALL_ACTORS}\n",
        "\n",
        "print(f\"Loaded {len(ALL_ACTORS)} actors\\n\")\n",
        "\n",
        "# LOAD SPACY\n",
        "print(\"Loading SpaCy English model...\")\n",
        "try:\n",
        "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "    print(\" SpaCy loaded successfully\")\n",
        "except:\n",
        "    print(\"Downloading SpaCy English model...\")\n",
        "    import subprocess\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "    print(\" SpaCy downloaded and loaded\")\n",
        "\n",
        "# LOAD MISTRAL\n",
        "print(\"\\nLoading Mistral 7B...\")\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "print(\" Mistral loaded\\n\")\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def fuzzy_match_actor(name, threshold=60):\n",
        "    if not name or len(name) < 2:\n",
        "        return None\n",
        "\n",
        "    name_lower = name.lower().strip()\n",
        "\n",
        "    if name_lower in ACTOR_LOWER_MAP:\n",
        "        return ACTOR_LOWER_MAP[name_lower]\n",
        "\n",
        "    result = process.extractOne(\n",
        "        name,\n",
        "        ALL_ACTORS,\n",
        "        scorer=fuzz.token_sort_ratio,\n",
        "        score_cutoff=threshold\n",
        "    )\n",
        "\n",
        "    if result:\n",
        "        matched_actor, score, _ = result\n",
        "        return matched_actor\n",
        "\n",
        "    return None\n",
        "\n",
        "def validate_triplet(triplet):\n",
        "    role_raw = triplet.get(\"role\", \"\")\n",
        "    counterrole_raw = triplet.get(\"counterrole\", \"\")\n",
        "    practice = triplet.get(\"practice\", \"\")\n",
        "\n",
        "    if not role_raw or not counterrole_raw or not practice:\n",
        "        return None, \"Missing fields\"\n",
        "\n",
        "    role_raw = role_raw.strip()\n",
        "    counterrole_raw = counterrole_raw.strip()\n",
        "    practice = practice.strip()\n",
        "\n",
        "    if len(role_raw) < 2 or len(counterrole_raw) < 3:\n",
        "        return None, \"Too short\"\n",
        "\n",
        "    counterrole_lower = counterrole_raw.lower()\n",
        "    generic_terms = [\n",
        "        \"us\", \"we\", \"you\", \"they\", \"them\", \"it\", \"this\", \"that\",\n",
        "        \"people\", \"businesses\", \"companies\", \"organizations\", \"partners\",\n",
        "        \"stakeholders\", \"members\", \"community\", \"projects\", \"programmes\",\n",
        "        \"opportunities\", \"solutions\", \"services\", \"products\",\n",
        "        \"Europe\", \"innovation\", \"development\", \"healthcare\", \"health\"\n",
        "    ]\n",
        "\n",
        "    if counterrole_lower in generic_terms:\n",
        "        return None, \"Generic counterrole\"\n",
        "\n",
        "    if len(counterrole_raw) > 100:\n",
        "        return None, \"Too long\"\n",
        "\n",
        "    vague_practices = [\"has\", \"is\", \"does\", \"makes\", \"gets\", \"uses\"]\n",
        "    if practice.lower() in vague_practices:\n",
        "        return None, \"Vague practice\"\n",
        "\n",
        "    role_matched = fuzzy_match_actor(role_raw, threshold=60)\n",
        "\n",
        "    if not role_matched:\n",
        "        return None, \"Role not in list\"\n",
        "\n",
        "    triplet[\"role\"] = role_matched\n",
        "    triplet[\"counterrole\"] = counterrole_raw\n",
        "\n",
        "    return triplet, \"valid\"\n",
        "\n",
        "def chunk_text(text, chunk_size=1500):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks, current = [], \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        if len(current) + len(sent) <= chunk_size:\n",
        "            current += sent + \" \"\n",
        "        else:\n",
        "            if current.strip():\n",
        "                chunks.append(current.strip())\n",
        "            current = sent + \" \"\n",
        "\n",
        "    if current.strip():\n",
        "        chunks.append(current.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def make_base_prompt(text_chunk, user_prompt=\"\"):\n",
        "    actor_examples = \"\\n\".join([f\"  • {actor}\" for actor in ALL_ACTORS[:30]])\n",
        "\n",
        "    base_prompt = f\"\"\"Extract organizational relationships as JSON.\n",
        "\n",
        "CONSTRAINT: 'role' MUST be from this list:\n",
        "{actor_examples}\n",
        "  • ... and {len(ALL_ACTORS)-30} more actors\n",
        "\n",
        "RULES:\n",
        "1. role: Organization taking action (from list above)\n",
        "2. practice: Specific action verb (e.g., \"fund\", \"partner with\", \"support\")\n",
        "3. counterrole: Specific named entity (not generic terms)\n",
        "4. context: Exact sentence\n",
        "\n",
        "Output ONLY valid JSON array.\"\"\"\n",
        "\n",
        "    if user_prompt and user_prompt.strip():\n",
        "        base_prompt += f\"\\n\\nADDITIONAL INSTRUCTIONS:\\n{user_prompt.strip()}\"\n",
        "\n",
        "    base_prompt += f\"\\n\\nTEXT:\\n{text_chunk}\\n\\nJSON:\"\n",
        "\n",
        "    return base_prompt\n",
        "\n",
        "# EXTRACTION FUNCTIONS\n",
        "def extract_with_mistral(text, user_prompt):\n",
        "    prompt = make_base_prompt(text, user_prompt)\n",
        "\n",
        "    try:\n",
        "        inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3000).to(mistral_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = mistral_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=700,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=mistral_tokenizer.eos_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "        result = mistral_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        result = result.replace(prompt, \"\").strip()\n",
        "\n",
        "        scores = outputs.scores\n",
        "        token_confidences = [torch.max(torch.nn.functional.softmax(score[0], dim=-1)).item() for score in scores]\n",
        "        avg_confidence = sum(token_confidences) / len(token_confidences) if token_confidences else 0.5\n",
        "\n",
        "        clean_output = result\n",
        "\n",
        "        if \"```json\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "        for prefix in [\"JSON:\", \"OUTPUT:\"]:\n",
        "            if clean_output.upper().startswith(prefix):\n",
        "                clean_output = clean_output[len(prefix):].strip()\n",
        "\n",
        "        start, end = clean_output.find(\"[\"), clean_output.rfind(\"]\")\n",
        "        if start == -1:\n",
        "            start, end = clean_output.find(\"{\"), clean_output.rfind(\"}\")\n",
        "            if start != -1 and end != -1:\n",
        "                clean_output = \"[\" + clean_output[start:end+1] + \"]\"\n",
        "\n",
        "        json_text = clean_output[start:end+1] if start != -1 and end != -1 else \"[]\"\n",
        "\n",
        "        triples = json.loads(json_text)\n",
        "        if isinstance(triples, dict):\n",
        "            triples = [triples]\n",
        "\n",
        "        validated_triples = []\n",
        "        for triple in triples:\n",
        "            triple['model_confidence'] = round(avg_confidence, 3)\n",
        "            triple.setdefault('role', '')\n",
        "            triple.setdefault('practice', '')\n",
        "            triple.setdefault('counterrole', '')\n",
        "            triple.setdefault('context', '')\n",
        "\n",
        "            validated, reason = validate_triplet(triple)\n",
        "\n",
        "            if validated:\n",
        "                validated_triples.append(validated)\n",
        "\n",
        "        return validated_triples\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "def extract_with_spacy_llm(text, user_prompt):\n",
        "    try:\n",
        "        doc = nlp_spacy(text[:5000])\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        potential_roles = []\n",
        "        text_lower = text.lower()\n",
        "        for actor in ALL_ACTORS:\n",
        "            if actor.lower() in text_lower:\n",
        "                potential_roles.append(actor)\n",
        "\n",
        "        if not potential_roles:\n",
        "            potential_roles = [\"EIT Health\", \"EIT Food\", \"EIT InnoEnergy\"]\n",
        "\n",
        "        verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"][:20]\n",
        "\n",
        "        spacy_enhanced_prompt = f\"\"\"Extract organizational relationships using NLP analysis:\n",
        "\n",
        "DETECTED ORGANIZATIONS: {', '.join(potential_roles[:10])}\n",
        "NAMED ENTITIES: {', '.join([f\"{e[0]} ({e[1]})\" for e in entities[:15]])}\n",
        "KEY VERBS: {', '.join(verbs[:15])}\n",
        "\n",
        "Task: Extract triplets (role → practice → counterrole)\n",
        "- role: MUST be from detected organizations above\n",
        "- practice: Action verb (preferably from key verbs)\n",
        "- counterrole: Specific named entity (from detected entities or text)\n",
        "- context: Exact sentence from text\n",
        "\n",
        "STRICT RULES:\n",
        "1. role must match detected organizations\n",
        "2. counterrole must be specific (NOT generic terms like \"partners\", \"stakeholders\")\n",
        "3. practice must be a clear action\n",
        "\n",
        "Text to analyze:\n",
        "{text[:2000]}\n",
        "\n",
        "Output ONLY JSON array:\n",
        "[{{\"role\": \"...\", \"practice\": \"...\", \"counterrole\": \"...\", \"context\": \"...\"}}]\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "        inputs = mistral_tokenizer(\n",
        "            spacy_enhanced_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=3000\n",
        "        ).to(mistral_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = mistral_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=700,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=mistral_tokenizer.eos_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "        result = mistral_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        result = result.replace(spacy_enhanced_prompt, \"\").strip()\n",
        "\n",
        "        scores = outputs.scores\n",
        "        token_confidences = [\n",
        "            torch.max(torch.nn.functional.softmax(score[0], dim=-1)).item()\n",
        "            for score in scores\n",
        "        ]\n",
        "        avg_confidence = sum(token_confidences) / len(token_confidences) if token_confidences else 0.75\n",
        "\n",
        "        clean_output = result\n",
        "        if \"```json\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "        for prefix in [\"JSON:\", \"OUTPUT:\"]:\n",
        "            if clean_output.upper().startswith(prefix):\n",
        "                clean_output = clean_output[len(prefix):].strip()\n",
        "\n",
        "        start, end = clean_output.find(\"[\"), clean_output.rfind(\"]\")\n",
        "        if start == -1:\n",
        "            start, end = clean_output.find(\"{\"), clean_output.rfind(\"}\")\n",
        "            if start != -1 and end != -1:\n",
        "                clean_output = \"[\" + clean_output[start:end+1] + \"]\"\n",
        "\n",
        "        json_text = clean_output[start:end+1] if start != -1 and end != -1 else \"[]\"\n",
        "\n",
        "        relations = json.loads(json_text)\n",
        "        if isinstance(relations, dict):\n",
        "            relations = [relations]\n",
        "\n",
        "        validated_triples = []\n",
        "        for rel in relations:\n",
        "            triplet = {\n",
        "                \"role\": rel.get(\"role\", \"\"),\n",
        "                \"practice\": rel.get(\"practice\", \"\"),\n",
        "                \"counterrole\": rel.get(\"counterrole\", \"\"),\n",
        "                \"context\": rel.get(\"context\", \"\"),\n",
        "                \"model_confidence\": round(avg_confidence, 3)\n",
        "            }\n",
        "\n",
        "            validated, reason = validate_triplet(triplet)\n",
        "            if validated:\n",
        "                validated_triples.append(validated)\n",
        "\n",
        "        return validated_triples\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SpaCy-LLM error: {e}\")\n",
        "        return []\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# FLASK APP\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/extract_triplets\", methods=[\"POST\"])\n",
        "def extract_endpoint():\n",
        "    data = request.get_json(force=True)\n",
        "    text = data.get(\"text\", \"\")\n",
        "    model_choice = data.get(\"model\", \"Mistral 7B\").strip()\n",
        "    user_prompt = data.get(\"user_prompt\", \"\")\n",
        "    max_triplets = data.get(\"max_triplets\", None)\n",
        "\n",
        "    if not text.strip():\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    model_map = {\n",
        "        \"Mistral 7B\": \"mistral\",\n",
        "        \"SpacyLLM\": \"spacy-llm\"\n",
        "    }\n",
        "\n",
        "    model_key = model_map.get(model_choice, \"mistral\")\n",
        "\n",
        "    model_names = {\n",
        "        \"mistral\": \"Mistral 7B\",\n",
        "        \"spacy-llm\": \"SpaCy-LLM (SpaCy NER + Mistral)\"\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EXTRACTION STARTED\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Text: {len(text):,} characters\")\n",
        "    print(f\"  Model: {model_names[model_key]}\")\n",
        "    print(f\"  User prompt: {'Yes' if user_prompt else 'No'}\")\n",
        "    print(f\"  Max triplets: {max_triplets if max_triplets else 'Unlimited'}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    chunks = chunk_text(text, chunk_size=1500)\n",
        "    total_chunks = len(chunks)\n",
        "\n",
        "    print(f\"Total chunks: {total_chunks}\")\n",
        "    print(f\"Estimated time: {total_chunks*2/60:.1f} minutes\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    all_triplets = []\n",
        "\n",
        "    health_end = 111637\n",
        "    food_end = health_end + 122735\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if max_triplets and len(all_triplets) >= max_triplets:\n",
        "            print(f\"\\n Reached max triplets limit: {max_triplets}\")\n",
        "            print(f\"  Stopping at chunk {i+1}/{total_chunks}\")\n",
        "            break\n",
        "\n",
        "        char_pos = i * 1500\n",
        "        if char_pos < health_end:\n",
        "            current_org = \"EIT Health\"\n",
        "        elif char_pos < food_end:\n",
        "            current_org = \"EIT Food\"\n",
        "        else:\n",
        "            current_org = \"EIT InnoEnergy\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"CHUNK {i+1}/{total_chunks} | {current_org}\")\n",
        "        print(f\"MODEL: {model_names[model_key]}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            if model_key == \"spacy-llm\":\n",
        "                triples = extract_with_spacy_llm(chunk, user_prompt)\n",
        "            else:\n",
        "                triples = extract_with_mistral(chunk, user_prompt)\n",
        "\n",
        "            if triples:\n",
        "                for triple in triples:\n",
        "                    if max_triplets and len(all_triplets) >= max_triplets:\n",
        "                        print(f\"\\n Reached limit during chunk processing\")\n",
        "                        break\n",
        "\n",
        "                    all_triplets.append(triple)\n",
        "\n",
        "                    role = triple.get('role', 'Unknown')\n",
        "                    practice = triple.get('practice', 'Unknown')\n",
        "                    counterrole = triple.get('counterrole', 'Unknown')\n",
        "\n",
        "                    print(f\"  {role} → {practice} → {counterrole}\")\n",
        "\n",
        "                if max_triplets and len(all_triplets) >= max_triplets:\n",
        "                    print(f\"\\n Stopping: reached {max_triplets} triplets\")\n",
        "                    break\n",
        "\n",
        "                print(f\"\\n Total so far: {len(all_triplets)} triplets\")\n",
        "            else:\n",
        "                print(f\"  No valid triplets found\")\n",
        "                print(f\" Total so far: {len(all_triplets)} triplets\")\n",
        "\n",
        "            time.sleep(0.3)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error: {str(e)[:100]}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" EXTRACTION COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Model: {model_names[model_key]}\")\n",
        "    print(f\"  Total triplets: {len(all_triplets)}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    formatted_triplets = []\n",
        "    for idx, triple in enumerate(all_triplets):\n",
        "        formatted_triplets.append({\n",
        "            \"id\": idx + 1,\n",
        "            \"text\": triple.get(\"context\", \"No context\"),\n",
        "            \"community\": \"EIT Community\",\n",
        "            \"extracted\": {\n",
        "                \"role\": triple.get(\"role\", \"Unknown\"),\n",
        "                \"practice\": triple.get(\"practice\", \"Unknown\"),\n",
        "                \"counterrole\": triple.get(\"counterrole\", \"Unknown\")\n",
        "            },\n",
        "            \"confidence\": triple.get(\"model_confidence\", 0.5),\n",
        "            \"validated\": None\n",
        "        })\n",
        "\n",
        "    return jsonify({\n",
        "        \"total_chunks\": total_chunks,\n",
        "        \"total_triplets\": len(formatted_triplets),\n",
        "        \"triplets\": formatted_triplets,\n",
        "        \"model_used\": model_names[model_key],\n",
        "        \"status\": \"success\"\n",
        "    })\n",
        "\n",
        "@app.route(\"/scrape_url\", methods=[\"POST\"])\n",
        "def scrape_endpoint():\n",
        "    \"\"\"Enhanced scraping with 1500 WORD limit\"\"\"\n",
        "    data = request.get_json(force=True)\n",
        "    url = data.get(\"url\", \"\")\n",
        "\n",
        "    if not url:\n",
        "        return jsonify({\"error\": \"No URL provided\"}), 400\n",
        "\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove unwanted elements\n",
        "        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'form', 'button']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Target main content area\n",
        "        main_content = (\n",
        "            soup.find('main') or\n",
        "            soup.find('article') or\n",
        "            soup.find(class_=['content', 'main-content', 'article-body', 'post-content']) or\n",
        "            soup.find(id=['content', 'main', 'article']) or\n",
        "            soup.body\n",
        "        )\n",
        "\n",
        "        if not main_content:\n",
        "            return jsonify({\"error\": \"Could not find main content\"}), 400\n",
        "\n",
        "        # Extract text from multiple elements\n",
        "        text_elements = []\n",
        "\n",
        "        # Headers (h1-h6)\n",
        "        for header in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
        "            text = header.get_text().strip()\n",
        "            if text and len(text) > 10:\n",
        "                text_elements.append(('header', text))\n",
        "\n",
        "        # Paragraphs\n",
        "        for p in main_content.find_all('p'):\n",
        "            text = p.get_text().strip()\n",
        "            if text and len(text) > 20:\n",
        "                text_elements.append(('paragraph', text))\n",
        "\n",
        "        # List items\n",
        "        for li in main_content.find_all('li'):\n",
        "            text = li.get_text().strip()\n",
        "            if text and len(text) > 15:\n",
        "                text_elements.append(('list', text))\n",
        "\n",
        "        # Divs with substantial text\n",
        "        for div in main_content.find_all('div'):\n",
        "            if len(div.find_all()) < 3:\n",
        "                text = div.get_text().strip()\n",
        "                if text and len(text) > 30 and len(text) < 500:\n",
        "                    text_elements.append(('div', text))\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        unique_texts = []\n",
        "\n",
        "        for element_type, text in text_elements:\n",
        "            normalized = ' '.join(text.split())\n",
        "\n",
        "            if normalized and normalized not in seen:\n",
        "                seen.add(normalized)\n",
        "                unique_texts.append(normalized)\n",
        "\n",
        "        # Join all text\n",
        "        full_text = \"\\n\\n\".join(unique_texts)\n",
        "\n",
        "        # Limit to 1500 WORDS\n",
        "        MAX_WORDS = 1500\n",
        "        words = full_text.split()\n",
        "\n",
        "        if len(words) > MAX_WORDS:\n",
        "            # Take first 1500 words\n",
        "            limited_words = words[:MAX_WORDS]\n",
        "\n",
        "            # Try to end at sentence boundary\n",
        "            cleaned_text = ' '.join(limited_words)\n",
        "\n",
        "            # Find last sentence ending\n",
        "            last_period = cleaned_text.rfind('.')\n",
        "            last_exclaim = cleaned_text.rfind('!')\n",
        "            last_question = cleaned_text.rfind('?')\n",
        "\n",
        "            cut_point = max(last_period, last_exclaim, last_question)\n",
        "\n",
        "            # Only cut at sentence if reasonable (within last 100 chars)\n",
        "            if cut_point > len(cleaned_text) - 100:\n",
        "                cleaned_text = cleaned_text[:cut_point + 1]\n",
        "            else:\n",
        "                cleaned_text = cleaned_text + \"...\"\n",
        "        else:\n",
        "            cleaned_text = full_text\n",
        "\n",
        "        word_count = len(cleaned_text.split())\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"✓ WEB SCRAPING COMPLETE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  URL: {url}\")\n",
        "        print(f\"  Text blocks extracted: {len(unique_texts)}\")\n",
        "        print(f\"  Total words (raw): {len(words)}\")\n",
        "        print(f\"  Words returned: {word_count}\")\n",
        "        print(f\"  Characters: {len(cleaned_text)}\")\n",
        "        print(f\"  Truncated: {'Yes' if len(words) > MAX_WORDS else 'No'}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return jsonify({\n",
        "            \"text\": cleaned_text,\n",
        "            \"source\": url,\n",
        "            \"stats\": {\n",
        "                \"total_blocks\": len(unique_texts),\n",
        "                \"total_words\": len(words),\n",
        "                \"returned_words\": word_count,\n",
        "                \"returned_chars\": len(cleaned_text),\n",
        "                \"truncated\": len(words) > MAX_WORDS\n",
        "            }\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/render_network\", methods=[\"POST\"])\n",
        "def render_network_endpoint():\n",
        "    csv_path = \"/content/gt_graph_embedded.csv\"\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        return jsonify({\"error\": f\"Upload gt_graph_embedded.csv to /content/ first\"}), 400\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"BUILDING NETWORK FROM CSV\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\" Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "        # Parse embeddings from JSON column\n",
        "        embeddings = np.vstack(df['embedding'].apply(json.loads))\n",
        "        print(f\" Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "        # Calculate similarity\n",
        "        print(\" Calculating similarity matrix...\")\n",
        "        sim = cosine_similarity(embeddings)\n",
        "        print(f\" Similarity range: {sim.min():.3f} to {sim.max():.3f}\")\n",
        "\n",
        "        # HIGHER threshold to reduce edges\n",
        "        non_diag_sims = sim[np.triu_indices_from(sim, k=1)]\n",
        "        SIM_THRESHOLD = np.percentile(non_diag_sims, 95)\n",
        "        print(f\" Threshold: {SIM_THRESHOLD:.3f} (95th percentile)\")\n",
        "\n",
        "        # Build graph\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # Add nodes\n",
        "        for idx, row in df.iterrows():\n",
        "            categories = str(row['category_groups_list']).split(',')\n",
        "            label = categories[0].strip()[:50] if categories else f\"Node_{idx}\"\n",
        "            G.add_node(idx, label=label)\n",
        "\n",
        "        print(f\" Added {G.number_of_nodes()} nodes\")\n",
        "\n",
        "        # Add edges with LIMIT\n",
        "        MAX_EDGES = 10000\n",
        "        edge_candidates = []\n",
        "\n",
        "        for i in range(len(df)):\n",
        "            for j in range(i + 1, len(df)):\n",
        "                if sim[i, j] >= SIM_THRESHOLD:\n",
        "                    edge_candidates.append((i, j, sim[i, j]))\n",
        "\n",
        "        # Sort by weight and take top MAX_EDGES\n",
        "        edge_candidates.sort(key=lambda x: x[2], reverse=True)\n",
        "        edge_candidates = edge_candidates[:MAX_EDGES]\n",
        "\n",
        "        for i, j, weight in edge_candidates:\n",
        "            G.add_edge(i, j, weight=float(weight))\n",
        "\n",
        "        print(f\" Added {G.number_of_edges()} edges\")\n",
        "\n",
        "        # If still too few edges, lower threshold\n",
        "        if G.number_of_edges() < 100:\n",
        "            SIM_THRESHOLD = np.percentile(non_diag_sims, 90)\n",
        "            print(f\"Few edges. Lowering to 90th percentile: {SIM_THRESHOLD:.3f}\")\n",
        "\n",
        "            G.clear_edges()\n",
        "            edge_candidates = []\n",
        "\n",
        "            for i in range(len(df)):\n",
        "                for j in range(i + 1, len(df)):\n",
        "                    if sim[i, j] >= SIM_THRESHOLD:\n",
        "                        edge_candidates.append((i, j, sim[i, j]))\n",
        "\n",
        "            edge_candidates.sort(key=lambda x: x[2], reverse=True)\n",
        "            edge_candidates = edge_candidates[:MAX_EDGES]\n",
        "\n",
        "            for i, j, weight in edge_candidates:\n",
        "                G.add_edge(i, j, weight=float(weight))\n",
        "\n",
        "            print(f\" Added {G.number_of_edges()} edges with lower threshold\")\n",
        "\n",
        "        # Community detection\n",
        "        print(\"Detecting communities...\")\n",
        "        part = (\n",
        "            community_louvain.best_partition(G, weight=\"weight\")\n",
        "            if G.number_of_edges() > 0\n",
        "            else {n: 0 for n in G.nodes()}\n",
        "        )\n",
        "\n",
        "        nx.set_node_attributes(G, part, \"cluster_id\")\n",
        "        num_communities = len(set(part.values()))\n",
        "        print(f\" Detected {num_communities} communities\")\n",
        "\n",
        "        # Color by community\n",
        "        node_colors = {\n",
        "            n: f\"hsl({int((part[n] * 360) / max(1, num_communities))}, 70%, 50%)\"\n",
        "            for n in G.nodes()\n",
        "        }\n",
        "\n",
        "        # Size by degree\n",
        "        node_sizes = {n: max(8, min(30, int(G.degree(n)) * 3)) for n in G.nodes()}\n",
        "\n",
        "        # Generate visualization\n",
        "        print(\"Generating visualization...\")\n",
        "        tmpdir = tempfile.mkdtemp(prefix=\"eit_sigma_\")\n",
        "        html_path = os.path.join(tmpdir, \"network.html\")\n",
        "\n",
        "        try:\n",
        "            Sigma.write_html(\n",
        "                G, html_path,\n",
        "                fullscreen=True,\n",
        "                node_color=node_colors,\n",
        "                node_size=node_sizes,\n",
        "                node_label=\"label\",\n",
        "                default_edge_type=\"curve\",\n",
        "                default_node_label_size=12,\n",
        "                clickable_edges=False,\n",
        "                node_border_color_from='node',\n",
        "                edge_color='#cccccc'\n",
        "            )\n",
        "\n",
        "            html = pathlib.Path(html_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "            print(f\"{'='*70}\")\n",
        "            print(\" NETWORK COMPLETE\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"  Nodes: {G.number_of_nodes()}\")\n",
        "            print(f\"  Edges: {G.number_of_edges()}\")\n",
        "            print(f\"  Communities: {num_communities}\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "\n",
        "            return jsonify({\n",
        "                \"html\": html,\n",
        "                \"num_nodes\": G.number_of_nodes(),\n",
        "                \"num_edges\": G.number_of_edges(),\n",
        "                \"num_communities\": num_communities,\n",
        "                \"similarity_threshold\": float(SIM_THRESHOLD),\n",
        "                \"source\": \"CSV embeddings (top similarities)\"\n",
        "            })\n",
        "\n",
        "        except Exception as viz_error:\n",
        "            print(f\"Visualization error: {viz_error}\")\n",
        "            print(\" Retrying with fewer edges...\")\n",
        "\n",
        "            G_small = nx.Graph()\n",
        "            for n in G.nodes():\n",
        "                G_small.add_node(n, **G.nodes[n])\n",
        "\n",
        "            # Take only top 5000 edges\n",
        "            edges_sorted = sorted(G.edges(data=True), key=lambda x: x[2].get('weight', 0), reverse=True)\n",
        "            for i, j, data in edges_sorted[:5000]:\n",
        "                G_small.add_edge(i, j, **data)\n",
        "\n",
        "            print(f\"Reduced to {G_small.number_of_edges()} edges\")\n",
        "\n",
        "            Sigma.write_html(\n",
        "                G_small, html_path,\n",
        "                fullscreen=True,\n",
        "                node_color=node_colors,\n",
        "                node_size=node_sizes,\n",
        "                node_label=\"label\",\n",
        "                default_edge_type=\"curve\",\n",
        "                default_node_label_size=12\n",
        "            )\n",
        "\n",
        "            html = pathlib.Path(html_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "            return jsonify({\n",
        "                \"html\": html,\n",
        "                \"num_nodes\": G_small.number_of_nodes(),\n",
        "                \"num_edges\": G_small.number_of_edges(),\n",
        "                \"num_communities\": num_communities,\n",
        "                \"similarity_threshold\": float(SIM_THRESHOLD),\n",
        "                \"source\": \"CSV embeddings (reduced)\"\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_details = traceback.format_exc()\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\" NETWORK ERROR\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(error_details)\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        return jsonify({\"error\": f\"Failed: {str(e)}\", \"details\": error_details}), 500\n",
        "\n",
        "@app.route(\"/models\", methods=[\"GET\"])\n",
        "def get_models():\n",
        "    return jsonify({\n",
        "        \"models\": [\n",
        "            {\"id\": \"Mistral 7B\", \"name\": \"Mistral 7B\", \"type\": \"LLM\"},\n",
        "            {\"id\": \"SpacyLLM\", \"name\": \"SpaCy-LLM Hybrid\", \"type\": \"Hybrid\"}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# START SERVER\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING SERVER...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "authtoken = userdata.get(\"NGROK\")\n",
        "ngrok.set_auth_token(authtoken)\n",
        "public_url = ngrok.connect(5050)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" BACKEND READY\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n PASTE THIS URL IN YOUR UI:\\n\")\n",
        "print(f\"   {public_url.public_url}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\n Quick Setup:\")\n",
        "print(\"   1. Copy the URL above\")\n",
        "print(\"   2. Open your HTML file\")\n",
        "print(\"   3. Find: const BACKEND_BASE = '...'\")\n",
        "print(\"   4. Paste the URL (no /endpoint needed)\")\n",
        "print(\"\\n Models: Mistral 7B | SpaCy-LLM\")\n",
        "print(\" Network: CSV embeddings (gt_graph_embedded.csv)\")\n",
        "print(\" Scraping: 1500 word limit, no duplicates\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "Thread(target=lambda: app.run(port=5050, debug=False, use_reloader=False)).start()\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nStopped.\")"
      ]
    }
  ]
}