{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxfW5pgN0Mve"
      },
      "outputs": [],
      "source": [
        "#!kill -9 $(lsof -t -i:5000) 2>/dev/null\n",
        "#!kill -9 $(lsof -t -i:5050) 2>/dev/null\n",
        "#!pkill -f flask 2>/dev/null || true\n",
        "#!pkill -f ngrok 2>/dev/null || true\n",
        "#print(\"Cleared old Flask and ngrok processes.\")\n",
        "\n",
        "!pip install -q flask flask-cors pyngrok transformers accelerate torch rapidfuzz spacy spacy-llm\n",
        "\n",
        "import torch, json, re, gc, time\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata\n",
        "from rapidfuzz import fuzz, process\n",
        "import spacy\n",
        "\n",
        "actor_lists = {\n",
        "    \"EIT Organizations\": [\n",
        "        \"European Institute of Innovation and Technology (EIT)\",\n",
        "        \"EIT Food\", \"EIT Health\", \"EIT InnoEnergy\", \"EIT Digital\",\n",
        "        \"EIT RawMaterials\", \"EIT Manufacturing\", \"EIT Urban Mobility\",\n",
        "        \"EIT Climate-KIC\", \"EIT Culture & Creativity\"\n",
        "    ],\n",
        "    \"Universities / Research Institutes\": [\n",
        "        \"KU Leuven\", \"KTH Royal Institute of Technology\", \"Universitat Politècnica de Catalunya\",\n",
        "        \"Eindhoven University of Technology (TU/e)\", \"Instituto Superior Técnico\",\n",
        "        \"ESADE Business School\", \"Politecnico di Torino\", \"École Polytechnique\",\n",
        "        \"Aalto University\", \"Université Paris Sciences et Lettres (PSL)\", \"Université Paris-Saclay\",\n",
        "        \"Grenoble INP Institute of Technology\", \"École des Ponts ParisTech (ENPC)\",\n",
        "        \"AGH University of Science and Technology\", \"Imperial College London\",\n",
        "        \"University of Oxford\", \"University of Cambridge\", \"IESE Business School\",\n",
        "        \"IMIM – Hospital del Mar Medical Research Institute\",\n",
        "        \"Delft University of Technology\", \"University of Porto\",\n",
        "        \"University of Debrecen\", \"University of Luxembourg\",\n",
        "        \"Leitat Technology Center\", \"Fraunhofer Institute\",\n",
        "        \"Technical University of Munich\", \"RWTH Aachen University\",\n",
        "        \"Czech Technical University in Prague\",\n",
        "        \"Uppsala University\", \"Tartu University Hospital\",\n",
        "        \"North Lisbon University Hospital Centre\", \"Hospital Clínic\",\n",
        "        \"University of Maastricht\", \"Eötvös Lorand University\", \"KTH Institute\",\n",
        "        \"University of Pécs\"\n",
        "    ],\n",
        "    \"Companies / Corporates\": [\n",
        "        \"Schneider Electric\", \"Siemens Healthineers\", \"Roche\", \"Bayer\", \"Sanofi\",\n",
        "        \"Johnson & Johnson\", \"IBM\", \"Philips\", \"Bosch\", \"Microsoft\", \"GE Healthcare\",\n",
        "        \"URGO Group\", \"Ferrer\", \"Matmut\", \"ABB\", \"ENGIE\", \"Airbus\", \"ArcelorMittal\",\n",
        "        \"Veolia\", \"TotalEnergies\", \"Nestlé\", \"Danone\", \"PepsiCo\", \"Unilever\",\n",
        "        \"Shell\", \"Vattenfall\", \"Iberdrola\", \"Enel\", \"Equinor\", \"Nokia\", \"Ericsson\"\n",
        "    ],\n",
        "    \"Startups / SMEs\": [\n",
        "        \"iLoF\", \"Sleepiz\", \"Optellum\", \"Idoven\", \"PIPRA\", \"Antegenes\",\n",
        "        \"Clinomic\", \"Unhindr\", \"Leuko\", \"Ochre Bio\",\n",
        "        \"Hearts Radiant\", \"Allelica\", \"SolasCure\", \"Peptomyc\",\n",
        "        \"Oxford Endovascular\", \"Tubulis\", \"SideROS\", \"Emperra\",\n",
        "        \"FasTeesH\", \"MEDIKURA\", \"SpinDiag\", \"Selio Medical\",\n",
        "        \"Damibu\", \"Telomium\", \"Tracegrow\", \"Entremo\",\n",
        "        \"Recycleye\", \"InnoTractor\", \"LMAD\", \"OvaExpert\",\n",
        "        \"Feno\", \"Ganymed Robotics\",\n",
        "        \"ABLE Human Motion\", \"FLOWTION\", \"SeizeIT\", \"NanoRacks\",\n",
        "        \"AMEN New Technologies\", \"CroíValve\"\n",
        "    ],\n",
        "    \"Government / Public Sector\": [\n",
        "        \"European Commission\", \"European Union\", \"European Parliament\",\n",
        "        \"European Investment Fund (EIF)\", \"European Investment Bank (EIB)\",\n",
        "        \"National Health Service (UK)\", \"Ministry of Human Capacities (Hungary)\",\n",
        "        \"German Federal Ministry of Education and Research\", \"City of Debrecen\",\n",
        "        \"Spanish Ministry of Science and Innovation\",\n",
        "        \"French Ministry of Higher Education and Research\",\n",
        "        \"Italian Ministry for Economic Development\",\n",
        "        \"Polish Ministry of Climate and Environment\",\n",
        "        \"European Council\", \"European Court of Auditors\"\n",
        "    ],\n",
        "    \"Networks / Consortia / Foundations\": [\n",
        "        \"European Battery Alliance\", \"European Youth Energy Network\",\n",
        "        \"CommUnity+\", \"Foundation for Management and Industrial Research (MIR)\",\n",
        "        \"Enterprise Europe Network\", \"HealthTech For Care\",\n",
        "        \"Venture Centre of Excellence (VCoE)\",\n",
        "        \"BioMed Alliance\", \"WE Health Consortium\", \"CLOSE Consortium\",\n",
        "        \"InnoEnergy Alliance\", \"RawMaterials Academy\", \"Urban Mobility Academy\",\n",
        "        \"Climate-KIC Alumni Association\", \"EIT Digital Alumni\", \"RIS Hub Network\",\n",
        "        \"Regional Innovation Scheme (RIS)\", \"EIT Community Booster\",\n",
        "        \"Supernovas Programme\", \"Fondation de l'Avenir\",\n",
        "        \"Polish Medical Mission\", \"BRIGHT Project Innovation Team\"\n",
        "    ],\n",
        "    \"Investors / Funding Bodies\": [\n",
        "        \"Santander InnoEnergy Climate Fund\", \"EBA Strategic Battery Materials Fund\",\n",
        "        \"Aescuvest Crowdfunding Platform\", \"Zafir Capital\", \"Alta Life Sciences\",\n",
        "        \"YES!Delft\", \"Startup Wise Guys\", \"Speedinvest\", \"Seedcamp\",\n",
        "        \"EASME – Executive Agency for SMEs\", \"Horizon Europe\",\n",
        "        \"European Innovation Council (EIC)\", \"Business Finland\",\n",
        "        \"Vinnova\", \"CDTI Spain\", \"Bpifrance\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "ALL_ACTORS = []\n",
        "for category_actors in actor_lists.values():\n",
        "    ALL_ACTORS.extend(category_actors)\n",
        "\n",
        "ACTOR_LOWER_MAP = {actor.lower(): actor for actor in ALL_ACTORS}\n",
        "\n",
        "print(f\"Loaded {len(ALL_ACTORS)} actors\\n\")\n",
        "\n",
        "\n",
        "# LOAD SPACY\n",
        "print(\"Loading SpaCy English model...\")\n",
        "try:\n",
        "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"✓ SpaCy loaded successfully\")\n",
        "except:\n",
        "    print(\"Downloading SpaCy English model...\")\n",
        "    import subprocess\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"✓ SpaCy downloaded and loaded\")\n",
        "\n",
        "# LOAD MISTRAL\n",
        "print(\"\\nLoading Mistral 7B...\")\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "print(\"Mistral loaded\")\n",
        "\n",
        "# LOAD GEMMA\n",
        "print(\"\\nLoading Gemma 7B...\")\n",
        "hf_token = userdata.get(\"HF_TOKEN2\")\n",
        "gemma_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"google/gemma-7b-it\",\n",
        "    token=hf_token\n",
        ")\n",
        "gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n",
        "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-7b-it\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    token=hf_token\n",
        ")\n",
        "print(\"Gemma loaded\\n\")\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def fuzzy_match_actor(name, threshold=60):\n",
        "    if not name or len(name) < 2:\n",
        "        return None\n",
        "\n",
        "    name_lower = name.lower().strip()\n",
        "\n",
        "    if name_lower in ACTOR_LOWER_MAP:\n",
        "        return ACTOR_LOWER_MAP[name_lower]\n",
        "\n",
        "    result = process.extractOne(\n",
        "        name,\n",
        "        ALL_ACTORS,\n",
        "        scorer=fuzz.token_sort_ratio,\n",
        "        score_cutoff=threshold\n",
        "    )\n",
        "\n",
        "    if result:\n",
        "        matched_actor, score, _ = result\n",
        "        return matched_actor\n",
        "\n",
        "    return None\n",
        "\n",
        "def validate_triplet(triplet):\n",
        "    role_raw = triplet.get(\"role\", \"\")\n",
        "    counterrole_raw = triplet.get(\"counterrole\", \"\")\n",
        "    practice = triplet.get(\"practice\", \"\")\n",
        "\n",
        "    if not role_raw or not counterrole_raw or not practice:\n",
        "        return None, \"Missing fields\"\n",
        "\n",
        "    role_raw = role_raw.strip()\n",
        "    counterrole_raw = counterrole_raw.strip()\n",
        "    practice = practice.strip()\n",
        "\n",
        "    if len(role_raw) < 2 or len(counterrole_raw) < 3:\n",
        "        return None, \"Too short\"\n",
        "\n",
        "    counterrole_lower = counterrole_raw.lower()\n",
        "    generic_terms = [\n",
        "        \"us\", \"we\", \"you\", \"they\", \"them\", \"it\", \"this\", \"that\",\n",
        "        \"people\", \"businesses\", \"companies\", \"organizations\", \"partners\",\n",
        "        \"stakeholders\", \"members\", \"community\", \"projects\", \"programmes\",\n",
        "        \"opportunities\", \"solutions\", \"services\", \"products\",\n",
        "        \"Europe\", \"innovation\", \"development\", \"healthcare\", \"health\"\n",
        "    ]\n",
        "\n",
        "    if counterrole_lower in generic_terms:\n",
        "        return None, \"Generic counterrole\"\n",
        "\n",
        "    if len(counterrole_raw) > 100:\n",
        "        return None, \"Too long\"\n",
        "\n",
        "    vague_practices = [\"has\", \"is\", \"does\", \"makes\", \"gets\", \"uses\"]\n",
        "    if practice.lower() in vague_practices:\n",
        "        return None, \"Vague practice\"\n",
        "\n",
        "    role_matched = fuzzy_match_actor(role_raw, threshold=60)\n",
        "\n",
        "    if not role_matched:\n",
        "        return None, \"Role not in list\"\n",
        "\n",
        "    triplet[\"role\"] = role_matched\n",
        "    triplet[\"counterrole\"] = counterrole_raw\n",
        "\n",
        "    return triplet, \"valid\"\n",
        "\n",
        "def chunk_text(text, chunk_size=1500):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks, current = [], \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        if len(current) + len(sent) <= chunk_size:\n",
        "            current += sent + \" \"\n",
        "        else:\n",
        "            if current.strip():\n",
        "                chunks.append(current.strip())\n",
        "            current = sent + \" \"\n",
        "\n",
        "    if current.strip():\n",
        "        chunks.append(current.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def make_base_prompt(text_chunk, user_prompt=\"\"):\n",
        "    actor_examples = \"\\n\".join([f\"  • {actor}\" for actor in ALL_ACTORS[:30]])\n",
        "\n",
        "    base_prompt = f\"\"\"Extract organizational relationships as JSON.\n",
        "\n",
        "CONSTRAINT: 'role' MUST be from this list:\n",
        "{actor_examples}\n",
        "  • ... and {len(ALL_ACTORS)-30} more actors\n",
        "\n",
        "RULES:\n",
        "1. role: Organization taking action (from list above)\n",
        "2. practice: Specific action verb (e.g., \"fund\", \"partner with\", \"support\")\n",
        "3. counterrole: Specific named entity (not generic terms)\n",
        "4. context: Exact sentence\n",
        "\n",
        "Output ONLY valid JSON array.\"\"\"\n",
        "\n",
        "    if user_prompt and user_prompt.strip():\n",
        "        base_prompt += f\"\\n\\nADDITIONAL INSTRUCTIONS:\\n{user_prompt.strip()}\"\n",
        "\n",
        "    base_prompt += f\"\\n\\nTEXT:\\n{text_chunk}\\n\\nJSON:\"\n",
        "\n",
        "    return base_prompt\n",
        "\n",
        "def make_gemma_prompt(text_chunk, user_prompt=\"\"):\n",
        "    base = make_base_prompt(text_chunk, user_prompt)\n",
        "    return f\"<start_of_turn>user\\n{base}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "# EXTRACTION FUNCTIONS\n",
        "def extract_with_llm(text, user_prompt, model_name, tokenizer, model):\n",
        "    if model_name == \"gemma\":\n",
        "        prompt = make_gemma_prompt(text, user_prompt)\n",
        "    else:\n",
        "        prompt = make_base_prompt(text, user_prompt)\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3000).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=700,\n",
        "                temperature = 0.5 if model_name == \"gemma\" else 0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "        result = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        result = result.replace(prompt, \"\").strip()\n",
        "\n",
        "        scores = outputs.scores\n",
        "        token_confidences = [torch.max(torch.nn.functional.softmax(score[0], dim=-1)).item() for score in scores]\n",
        "        avg_confidence = sum(token_confidences) / len(token_confidences) if token_confidences else 0.5\n",
        "\n",
        "        clean_output = result\n",
        "\n",
        "        if \"```json\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "        for prefix in [\"JSON:\", \"OUTPUT:\"]:\n",
        "            if clean_output.upper().startswith(prefix):\n",
        "                clean_output = clean_output[len(prefix):].strip()\n",
        "\n",
        "        start, end = clean_output.find(\"[\"), clean_output.rfind(\"]\")\n",
        "        if start == -1:\n",
        "            start, end = clean_output.find(\"{\"), clean_output.rfind(\"}\")\n",
        "            if start != -1 and end != -1:\n",
        "                clean_output = \"[\" + clean_output[start:end+1] + \"]\"\n",
        "\n",
        "        json_text = clean_output[start:end+1] if start != -1 and end != -1 else \"[]\"\n",
        "\n",
        "        triples = json.loads(json_text)\n",
        "        if isinstance(triples, dict):\n",
        "            triples = [triples]\n",
        "\n",
        "        validated_triples = []\n",
        "        for triple in triples:\n",
        "            triple['model_confidence'] = round(avg_confidence, 3)\n",
        "            triple.setdefault('role', '')\n",
        "            triple.setdefault('practice', '')\n",
        "            triple.setdefault('counterrole', '')\n",
        "            triple.setdefault('context', '')\n",
        "\n",
        "            validated, reason = validate_triplet(triple)\n",
        "\n",
        "            if validated:\n",
        "                validated_triples.append(validated)\n",
        "\n",
        "        return validated_triples\n",
        "\n",
        "    except Exception as e:\n",
        "        return []\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "def extract_with_spacy_llm(text, user_prompt):\n",
        "    \"\"\"\n",
        "    TRUE SpaCy-LLM Hybrid:\n",
        "    1. SpaCy does NLP preprocessing (tokenization, NER, POS tagging)\n",
        "    2. Mistral does relation extraction with entity context\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = nlp_spacy(text[:5000])  # Limit to 5000 chars for speed\n",
        "\n",
        "        # Extract named entities\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        potential_roles = []\n",
        "        text_lower = text.lower()\n",
        "        for actor in ALL_ACTORS:\n",
        "            if actor.lower() in text_lower:\n",
        "                potential_roles.append(actor)\n",
        "\n",
        "        if not potential_roles:\n",
        "            potential_roles = [\"EIT Health\", \"EIT Food\", \"EIT InnoEnergy\"]\n",
        "\n",
        "        # Extract verbs that might be practices\n",
        "        verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"][:20]\n",
        "\n",
        "        spacy_enhanced_prompt = f\"\"\"Extract organizational relationships using NLP analysis:\n",
        "\n",
        "DETECTED ORGANIZATIONS: {', '.join(potential_roles[:10])}\n",
        "NAMED ENTITIES: {', '.join([f\"{e[0]} ({e[1]})\" for e in entities[:15]])}\n",
        "KEY VERBS: {', '.join(verbs[:15])}\n",
        "\n",
        "Task: Extract triplets (role → practice → counterrole)\n",
        "- role: MUST be from detected organizations above\n",
        "- practice: Action verb (preferably from key verbs)\n",
        "- counterrole: Specific named entity (from detected entities or text)\n",
        "- context: Exact sentence from text\n",
        "\n",
        "STRICT RULES:\n",
        "1. role must match detected organizations\n",
        "2. counterrole must be specific (NOT generic terms like \"partners\", \"stakeholders\")\n",
        "3. practice must be a clear action\n",
        "\n",
        "Text to analyze:\n",
        "{text[:2000]}\n",
        "\n",
        "Output ONLY JSON array:\n",
        "[{{\"role\": \"...\", \"practice\": \"...\", \"counterrole\": \"...\", \"context\": \"...\"}}]\n",
        "\n",
        "JSON:\"\"\"\n",
        "\n",
        "        # Mistral for relation extraction with SpaCy context\n",
        "        inputs = mistral_tokenizer(\n",
        "            spacy_enhanced_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=3000\n",
        "        ).to(mistral_model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = mistral_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=700,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=mistral_tokenizer.eos_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "        result = mistral_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        result = result.replace(spacy_enhanced_prompt, \"\").strip()\n",
        "\n",
        "        scores = outputs.scores\n",
        "        token_confidences = [\n",
        "            torch.max(torch.nn.functional.softmax(score[0], dim=-1)).item()\n",
        "            for score in scores\n",
        "        ]\n",
        "        avg_confidence = sum(token_confidences) / len(token_confidences) if token_confidences else 0.75\n",
        "\n",
        "        # Clean JSON output\n",
        "        clean_output = result\n",
        "        if \"```json\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in clean_output:\n",
        "            clean_output = clean_output.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "        for prefix in [\"JSON:\", \"OUTPUT:\"]:\n",
        "            if clean_output.upper().startswith(prefix):\n",
        "                clean_output = clean_output[len(prefix):].strip()\n",
        "\n",
        "        start, end = clean_output.find(\"[\"), clean_output.rfind(\"]\")\n",
        "        if start == -1:\n",
        "            start, end = clean_output.find(\"{\"), clean_output.rfind(\"}\")\n",
        "            if start != -1 and end != -1:\n",
        "                clean_output = \"[\" + clean_output[start:end+1] + \"]\"\n",
        "\n",
        "        json_text = clean_output[start:end+1] if start != -1 and end != -1 else \"[]\"\n",
        "\n",
        "        relations = json.loads(json_text)\n",
        "        if isinstance(relations, dict):\n",
        "            relations = [relations]\n",
        "\n",
        "        validated_triples = []\n",
        "        for rel in relations:\n",
        "            triplet = {\n",
        "                \"role\": rel.get(\"role\", \"\"),\n",
        "                \"practice\": rel.get(\"practice\", \"\"),\n",
        "                \"counterrole\": rel.get(\"counterrole\", \"\"),\n",
        "                \"context\": rel.get(\"context\", \"\"),\n",
        "                \"model_confidence\": round(avg_confidence, 3)\n",
        "            }\n",
        "\n",
        "            validated, reason = validate_triplet(triplet)\n",
        "            if validated:\n",
        "                validated_triples.append(validated)\n",
        "\n",
        "        return validated_triples\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SpaCy-LLM error: {e}\")\n",
        "        return []\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# FLASK APP\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/extract_triplets\", methods=[\"POST\"])\n",
        "def extract_endpoint():\n",
        "    data = request.get_json(force=True)\n",
        "    text = data.get(\"text\", \"\")\n",
        "    model_choice = data.get(\"model\", \"Mistral 7B\").strip()\n",
        "    user_prompt = data.get(\"user_prompt\", \"\")\n",
        "    max_triplets = data.get(\"max_triplets\", None)\n",
        "\n",
        "    if not text.strip():\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    model_map = {\n",
        "        \"Mistral 7B\": \"mistral\",\n",
        "        \"Gemma 7B\": \"gemma\",\n",
        "        \"SpacyLLM\": \"spacy-llm\"\n",
        "    }\n",
        "\n",
        "    model_key = model_map.get(model_choice, \"mistral\")\n",
        "\n",
        "    model_names = {\n",
        "        \"mistral\": \"Mistral 7B\",\n",
        "        \"gemma\": \"Gemma 7B\",\n",
        "        \"spacy-llm\": \"SpaCy-LLM (SpaCy NER + Mistral Extraction)\"\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CONFIGURATION:\")\n",
        "    print(f\"  Received: {len(text):,} characters\")\n",
        "    print(f\"  Model selected: {model_names[model_key]}\")\n",
        "    print(f\"  Model key: {model_key}\")\n",
        "    print(f\"  User prompt: {'Yes' if user_prompt else 'No'}\")\n",
        "    print(f\"  Max triplets: {max_triplets if max_triplets else 'Unlimited'}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    chunks = chunk_text(text, chunk_size=1500)\n",
        "    total_chunks = len(chunks)\n",
        "\n",
        "    print(f\"Total chunks: {total_chunks}\")\n",
        "    print(f\"Estimated time: {total_chunks*2/60:.1f} minutes\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    all_triplets = []\n",
        "\n",
        "    health_end = 111637\n",
        "    food_end = health_end + 122735\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if max_triplets and len(all_triplets) >= max_triplets:\n",
        "            print(f\"\\nReached max triplets limit: {max_triplets}\")\n",
        "            print(f\"Stopping at chunk {i+1}/{total_chunks}\")\n",
        "            break\n",
        "\n",
        "        char_pos = i * 1500\n",
        "        if char_pos < health_end:\n",
        "            current_org = \"EIT Health\"\n",
        "        elif char_pos < food_end:\n",
        "            current_org = \"EIT Food\"\n",
        "        else:\n",
        "            current_org = \"EIT InnoEnergy\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"CHUNK {i+1}/{total_chunks} | {current_org}\")\n",
        "        print(f\"MODEL: {model_names[model_key]}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            if model_key == \"spacy-llm\":\n",
        "                triples = extract_with_spacy_llm(chunk, user_prompt)\n",
        "            elif model_key == \"gemma\":\n",
        "                triples = extract_with_llm(chunk, user_prompt, \"gemma\", gemma_tokenizer, gemma_model)\n",
        "            else:\n",
        "                triples = extract_with_llm(chunk, user_prompt, \"mistral\", mistral_tokenizer, mistral_model)\n",
        "\n",
        "            if triples:\n",
        "                for triple in triples:\n",
        "                    if max_triplets and len(all_triplets) >= max_triplets:\n",
        "                        print(f\"\\nReached limit during chunk processing\")\n",
        "                        break\n",
        "\n",
        "                    all_triplets.append(triple)\n",
        "\n",
        "                    role = triple.get('role', 'Unknown')\n",
        "                    practice = triple.get('practice', 'Unknown')\n",
        "                    counterrole = triple.get('counterrole', 'Unknown')\n",
        "\n",
        "                    print(f\"{role} -> {practice} -> {counterrole}\")\n",
        "\n",
        "                if max_triplets and len(all_triplets) >= max_triplets:\n",
        "                    print(f\"\\nStopping: reached {max_triplets} triplets\")\n",
        "                    break\n",
        "\n",
        "                print(f\"\\nTotal so far: {len(all_triplets)} triplets\")\n",
        "            else:\n",
        "                print(f\"No valid triplets\")\n",
        "                print(f\"Total so far: {len(all_triplets)} triplets\")\n",
        "\n",
        "            time.sleep(0.3)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)[:100]}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EXTRACTION COMPLETE with {model_names[model_key]}!\")\n",
        "    print(f\"Total triplets: {len(all_triplets)}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    formatted_triplets = []\n",
        "    for idx, triple in enumerate(all_triplets):\n",
        "        formatted_triplets.append({\n",
        "            \"id\": idx + 1,\n",
        "            \"text\": triple.get(\"context\", \"No context\"),\n",
        "            \"community\": \"EIT Community\",\n",
        "            \"extracted\": {\n",
        "                \"role\": triple.get(\"role\", \"Unknown\"),\n",
        "                \"practice\": triple.get(\"practice\", \"Unknown\"),\n",
        "                \"counterrole\": triple.get(\"counterrole\", \"Unknown\")\n",
        "            },\n",
        "            \"confidence\": triple.get(\"model_confidence\", 0.5),\n",
        "            \"validated\": None\n",
        "        })\n",
        "\n",
        "    return jsonify({\n",
        "        \"total_chunks\": total_chunks,\n",
        "        \"total_triplets\": len(formatted_triplets),\n",
        "        \"triplets\": formatted_triplets,\n",
        "        \"model_used\": model_names[model_key],\n",
        "        \"status\": \"success\"\n",
        "    })\n",
        "\n",
        "@app.route(\"/models\", methods=[\"GET\"])\n",
        "def get_models():\n",
        "    return jsonify({\n",
        "        \"models\": [\n",
        "            {\"id\": \"Mistral 7B\", \"name\": \"Mistral 7B\", \"type\": \"LLM\"},\n",
        "            {\"id\": \"Gemma 7B\", \"name\": \"Gemma 7B\", \"type\": \"LLM\"},\n",
        "            {\"id\": \"SpacyLLM\", \"name\": \"SpaCy-LLM Hybrid\", \"type\": \"Hybrid\"}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# START SERVER\n",
        "print(\"\\nStarting server...\\n\")\n",
        "\n",
        "authtoken = userdata.get(\"NGROK\")\n",
        "ngrok.set_auth_token(authtoken)\n",
        "public_url = ngrok.connect(5050)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Models: {public_url.public_url}/models\")\n",
        "print(f\"Extract: {public_url.public_url}/extract_triplets\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAvailable models:\")\n",
        "print(\" Mistral 7B\")\n",
        "print(\" Gemma 7B\")\n",
        "print(\" SpaCy-LLM\")\n",
        "print(\"\\nReady!\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "Thread(target=lambda: app.run(port=5050, debug=False, use_reloader=False)).start()\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nStopped.\")"
      ]
    }
  ]
}